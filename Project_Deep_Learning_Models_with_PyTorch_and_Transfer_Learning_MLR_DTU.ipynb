{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sneharc16/DTU-MLR-Assignments-Deep-Learning/blob/main/Project_Deep_Learning_Models_with_PyTorch_and_Transfer_Learning_MLR_DTU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Dy_hZxXleu1"
      },
      "source": [
        "**initial notes**\n",
        "\n",
        "This is currently a work in progress. Take mnist data set from torrch vision lib then create a data loader for loading the dataset\n",
        "\n",
        "after data loading, develop a deep learning models using pytorch\n",
        "\n",
        "(data loader must have commenets as well, which is path which is train test split)\n",
        "\n",
        "model 1\n",
        "model with fully connected layers only\n",
        "(activations are allow, sigmoid softmpa)(no convulutions)\n",
        "(in channel and out cahnnels must be metnioned after every layer)\n",
        "\n",
        "\n",
        "model 2\n",
        "with convulusion layers followed by a fully connected layer\n",
        "(conv 2d and fully connected)\n",
        "(same comment wala standard)\n",
        "(in channel and out cahnnels must be metnioned after every layer)\n",
        "(write about bias as well)\n",
        "\n",
        "model 3\n",
        "model with only convulution layers\n",
        "(no fully connected layers)\n",
        "\n",
        "\n",
        "\n",
        "train the model using default train fucntion and with help of customised trainng loop as well\n",
        "\n",
        "default means = train function of python\n",
        "\n",
        "look for train loop as well\n",
        "\n",
        "\n",
        "\n",
        "furthur wrwite a function for loading a saved model, and performing/ fine tuning on that model\n",
        "\n",
        "apply k full-cross validation\n",
        "\n",
        "to check the perfomrane of developed model in differenet folds\n",
        "\n",
        "\n",
        "make a customised data laoder for a dog cat classifier\n",
        "\n",
        "dont maek this model from scrat, use the model from mnist here for the dog-cat classifier\n",
        "(this is basics of tranfer learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYnSoKS-leu6"
      },
      "source": [
        "\\**Project: Deep Learning Models with PyTorch and Transfer Learning**\n",
        "\n",
        "**Objective:** Develop three different deep learning models using the MNIST dataset and PyTorch, train them using both a default training function and a custom training loop, and apply transfer learning to a new task.\n",
        "\n",
        "**Part 1: Data Preparation**\n",
        "\n",
        "1. Import the MNIST dataset from the torchvision library.\n",
        "2. Create a DataLoader for loading the dataset. Include comments in your code explaining the path and train/test split.\n",
        "\n",
        "**Part 2: Model Development**\n",
        "\n",
        "Develop three different models as follows:\n",
        "\n",
        "**Model 1: Fully Connected Neural Network**\n",
        "\n",
        "- Construct a model using only fully connected layers.\n",
        "- You may use activation functions such as sigmoid and softmax, but do not use convolutional layers.\n",
        "- After each layer, include comments in your code specifying the number of input and output channels.\n",
        "\n",
        "**Model 2: Convolutional Neural Network with Fully Connected Layer**\n",
        "\n",
        "- Construct a model that includes convolutional layers (Conv2D) followed by a fully connected layer.\n",
        "- After each layer, include comments in your code specifying the number of input and output channels.\n",
        "- Also, include comments about the bias in each layer.\n",
        "\n",
        "**Model 3: Convolutional Neural Network**\n",
        "\n",
        "- Construct a model using only convolutional layers (Conv2D).\n",
        "- Do not include any fully connected layers in this model.\n",
        "\n",
        "**Part 3: Model Training**\n",
        "\n",
        "- Train each model using the default training function provided by PyTorch.\n",
        "- Also, develop a custom training loop and use it to train each model.\n",
        "- Compare the performance of the models when trained with the default function versus the custom loop.\n",
        "\n",
        "**Part 4: Model Evaluation**\n",
        "\n",
        "- Write a function for loading a saved model and performing fine-tuning on that model.\n",
        "- Apply k-fold cross-validation to check the performance of the developed models in different folds.\n",
        "\n",
        "**Part 5: Transfer Learning**\n",
        "\n",
        "- Create a custom DataLoader for a dog-cat classifier.\n",
        "- Don't build this model from scratch. Instead, use one of the models trained on the MNIST dataset and fine-tune it for the dog-cat classification task. This is an example of transfer learning.\n",
        "\n",
        "Remember to follow best practices for deep learning model development, including appropriate use of training/validation/test splits, normalization of input data, and regularization to prevent overfitting. Good luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tqkVCyqleu6"
      },
      "source": [
        "# code below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYKDMZpaleu6"
      },
      "source": [
        "These are the packages which will be required thorught the project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aijTb4SXleu7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import os\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import pickle\n",
        "from sklearn.model_selection import KFold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFc0sIKQleu8"
      },
      "source": [
        "# Data Loading\n",
        "**Importing Dataset**\n",
        "\n",
        "I have used in-built data loader of pytorch via which I have divided the data into to train and test as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoZISvf2leu8",
        "outputId": "9e579c94-c97f-4d20-c42d-c62518a57e48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 18.0MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 486kB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.56MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 9.69MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "train_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        "    )\n",
        "\n",
        "test_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AaIq0GBsleu9"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "loaders = {\n",
        "    'train': torch.utils.data.DataLoader(train_data, batch_size=100, shuffle=True, num_workers=2),\n",
        "    'test': torch.utils.data.DataLoader(test_data, batch_size=100, shuffle=True, num_workers=2)\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGZHFx9Lleu9"
      },
      "source": [
        "# Model Developement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2Wkllagleu9"
      },
      "source": [
        "**Fully connected Neural Netwrok**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VUdZNgzDleu-"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FullyConnectedModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FullyConnectedModel, self).__init__()\n",
        "\n",
        "        # Input layer: 784 input channels (for a 28x28 image), 512 output channels\n",
        "        self.fc1 = nn.Linear(784, 512)\n",
        "\n",
        "        # Hidden layer: 512 input channels, 256 output channels\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "\n",
        "        # Output layer: 256 input channels, 10 output channels (for 10 classes)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten the input from a 28x28 image to a 784-dim vector\n",
        "        x = x.view(-1, 784)\n",
        "\n",
        "        # Apply ReLU activation function after first and second fully connected layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "        # Apply Softmax activation function after third fully connected layer\n",
        "        x = F.softmax(self.fc3(x), dim=1)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwRKRWC6leu-"
      },
      "source": [
        "**Hybrid model with both fully layers and convulutional layers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jXtPGq5Jleu-"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "\n",
        "        # Convolutional layer: 1 input channel (grayscale image), 6 output channels, 3x3 kernel\n",
        "        # Bias is included by default\n",
        "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
        "\n",
        "        # Convolutional layer: 6 input channels, 16 output channels, 3x3 kernel\n",
        "        # Bias is included by default\n",
        "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
        "\n",
        "        # Fully connected layer: 16*6*6 input channels (from the conv2 output), 120 output channels\n",
        "        # Bias is included by default\n",
        "        self.fc1 = nn.Linear(16 * 6 * 6, 120)\n",
        "\n",
        "        # Fully connected layer: 120 input channels, 84 output channels\n",
        "        # Bias is included by default\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "\n",
        "        # Output layer: 84 input channels, 10 output channels (for 10 classes)\n",
        "        # Bias is included by default\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply ReLU activation function after each convolutional layer\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "\n",
        "        # Flatten the tensor output from the convolutional layers\n",
        "        x = x.view(-1, 16 * 6 * 6)\n",
        "\n",
        "        # Apply ReLU activation function after each fully connected layer\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "        # Apply Softmax activation function after output layer\n",
        "        x = F.softmax(self.fc3(x), dim=1)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqK3Wxwzleu-"
      },
      "source": [
        "**Model made only from convulutional layers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Sz73QPEOleu-"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "\n",
        "        # Convolutional layer: 1 input channel (grayscale image), 6 output channels, 3x3 kernel\n",
        "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
        "\n",
        "        # Convolutional layer: 6 input channels, 16 output channels, 3x3 kernel\n",
        "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
        "\n",
        "        # Convolutional layer: 16 input channels, 32 output channels, 3x3 kernel\n",
        "        self.conv3 = nn.Conv2d(16, 32, 3)\n",
        "\n",
        "        # Convolutional layer: 32 input channels, 64 output channels, 3x3 kernel\n",
        "        self.conv4 = nn.Conv2d(32, 64, 3)\n",
        "\n",
        "        # Output layer: 64 input channels, 10 output channels (for 10 classes), 1x1 kernel\n",
        "        self.conv5 = nn.Conv2d(64, 10, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply ReLU activation function after each convolutional layer\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "\n",
        "        # Apply Softmax activation function after output layer\n",
        "        x = F.softmax(self.conv5(x), dim=1)\n",
        "\n",
        "        # Flatten the tensor output from the convolutional layers\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXj-20udleu_"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_eUvXtRleu_"
      },
      "source": [
        "Testing for CUDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfwDxqpHleu_",
        "outputId": "eb8f516b-5cca-49be-bbe7-47f615091b1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_agIwFWleu_"
      },
      "source": [
        "Model made with only Conv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ujTVuglWlevA"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the ConvNet model (defined elsewhere)\n",
        "model = ConvNet().to(device)\n",
        "\n",
        "# Define the optimizer (Adam optimizer with learning rate 0.001)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define the loss function (CrossEntropyLoss for multi-class classification)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def train(epoch):\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(loaders['train']):\n",
        "    # Move data and target tensors to the chosen device (CPU or GPU)\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # Zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    #Perform a forward pass through the model to get predictions\n",
        "    output = model(data)\n",
        "\n",
        "    #Calculate the loss based on prediction and target labels\n",
        "    loss = loss_fn(output, target)\n",
        "\n",
        "    #Backpropagate the loss to calculate gradients for each parameter\n",
        "    loss.backward()\n",
        "\n",
        "    # Update model parameters based on the calculated gradients using the optimizer\n",
        "    optimizer.step()\n",
        "    if batch_idx % 100 == 0:\n",
        "\n",
        "      # Print training progress information every 100 batches\n",
        "      print (f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(loaders[\"train\"].dataset)} ({100. * batch_idx / len(loaders[\"train\"]):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "\n",
        "def test():\n",
        "  model.eval() # Set model to evaluation mode\n",
        "\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "\n",
        "   # Disable gradient calculation for efficiency during testing\n",
        "  with torch.no_grad():\n",
        "    for data, target in loaders['test']:\n",
        "      \"\"\"\n",
        "      Iterates over batches of data from the 'test' dataloader.\n",
        "\n",
        "      Args:\n",
        "          data: A batch of test images.\n",
        "          target: A batch of corresponding labels for the test images.\n",
        "      \"\"\"\n",
        "\n",
        "      # Move data and target tensors to the chosen device (CPU or GPU)\n",
        "      data, target = data.to(device), target.to(device)\n",
        "\n",
        "      # Get predictions from the model\n",
        "      output = model(data)\n",
        "\n",
        "      # Accumulate the loss for the entire test set\n",
        "      test_loss += loss_fn(output, target)\n",
        "\n",
        "      # Get the predicted class with the highest probability for each image\n",
        "      pred = output.max(1, keepdim=True)[1]\n",
        "\n",
        "      # Calculate the number of correct predictions\n",
        "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "  # Calculate average test loss\n",
        "  test_loss /= len(loaders['test'].dataset)\n",
        "\n",
        "  # Print accuracy and loss for the entire test set\n",
        "  print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    test_loss, correct, len(loaders['test'].dataset),\n",
        "    100. * correct / len(loaders['test'].dataset)))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "2WRGLMg9levA",
        "outputId": "3b2fd61b-9701-4854-dbeb-bc65642a5aed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 8.304434\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-f29aa4830874>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-49c24472adfc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m#Backpropagate the loss to calculate gradients for each parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Update model parameters based on the calculated gradients using the optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for epoch in range(1, 5):\n",
        "  train(epoch)\n",
        "  test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "OkFj2y89levA",
        "outputId": "74651cbb-58e8-479c-b268-07c8ec89c27c"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e01b0e5b2db2>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model.eval()\n",
        "\n",
        "data, target = test_data[3]\n",
        "\n",
        "data = data.unsqueeze(0)\n",
        "\n",
        "output = model(data)\n",
        "\n",
        "pred = output.max(1, keepdim=True)[1]\n",
        "\n",
        "print(pred)\n",
        "\n",
        "image = data.squeeze().numpy()\n",
        "\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.show"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-c6M7iCRrZ-i"
      },
      "source": [
        "##KNN##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "qpV55WbUncJO",
        "outputId": "79997d1b-7ca5-4299-a3d0-44050852e2de"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "k-fold cross-validation requires at least one train/test split by setting n_splits=2 or more, got n_splits=1.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-6096d2cef47e>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Define KFold object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnum_folds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Adjust the number of folds to be at most the number of samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mkfold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Loop over folds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n_splits, shuffle, random_state)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_iter_test_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n_splits, shuffle, random_state)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_splits\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    299\u001b[0m                 \u001b[0;34m\"k-fold cross-validation requires at least one\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m                 \u001b[0;34m\" train/test split by setting n_splits=2 or more,\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: k-fold cross-validation requires at least one train/test split by setting n_splits=2 or more, got n_splits=1."
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import numpy as np # Import numpy\n",
        "\n",
        "# Define KFold object\n",
        "num_folds = min(5, len(data))  # Adjust the number of folds to be at most the number of samples\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
        "\n",
        "# Loop over folds\n",
        "for train_index, test_index in kfold.split(data):\n",
        "  # Create training and validation sets from indices\n",
        "  X_train_fold, X_val_fold = np.array(data)[train_index], np.array(target)[test_index] # Convert data to a NumPy array for proper indexing\n",
        "  y_train_fold, y_val_fold = np.array(target)[train_index], np.array(target)[test_index] # Convert target to a NumPy array for proper indexing\n",
        "\n",
        "\n",
        "  # Train the model on the training fold data\n",
        "  model = ConvNet().to(device)  # Re-initialize model for each fold\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "  def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(loaders['train_fold']):\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      output = model(data)\n",
        "      loss = loss_fn(output, target)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      if batch_idx % 100 == 0:\n",
        "        print (f'Train Fold {train_index+1} Epoch: {epoch} [{batch_idx * len(data)}/{len(loaders[\"train_fold\"].dataset)} ({100. * batch_idx / len(loaders[\"train_fold\"]):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "\n",
        "  def test(model, data_loader, name):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "      for data, target in data_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        test_loss += loss_fn(output, target)\n",
        "        pred = output.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    test_loss /= len(data_loader.dataset)\n",
        "    print('\\n{} set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "      name, test_loss, correct, len(data_loader.dataset),\n",
        "      100. * correct / len(data_loader.dataset)))\n",
        "\n",
        "  # Train the model\n",
        "  for epoch in range(1, 5):\n",
        "    train(epoch)\n",
        "\n",
        "  # Evaluate the model on the validation fold data\n",
        "  test(model, loaders['val_fold'], 'Val')\n",
        "\n",
        "# After iterating through all folds, you can calculate average metrics across folds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dixBLjrFvo6"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the number of splits\n",
        "n_splits = 10\n",
        "\n",
        "# Create a KFold object\n",
        "kfold = KFold(n_splits=n_splits, shuffle=True)\n",
        "\n",
        "# Convert the train_data to a tensor\n",
        "train_data_tensor = torch.tensor(train_data.data.numpy())\n",
        "\n",
        "# Define the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Loop through the indices returned by kfold.split()\n",
        "for fold, (train_ids, test_ids) in enumerate(kfold.split(train_data_tensor)):\n",
        "    print(f'FOLD {fold}')\n",
        "    print('--------------------------------')\n",
        "\n",
        "    # Create Subset of the data for this fold\n",
        "    train_subs = Subset(train_data, train_ids)\n",
        "    test_subs = Subset(train_data, test_ids)\n",
        "\n",
        "    # Define data loaders for training and testing data in this fold\n",
        "    trainloader = DataLoader(train_subs, batch_size=100, shuffle=True)\n",
        "    testloader = DataLoader(test_subs, batch_size=100, shuffle=True)\n",
        "\n",
        "    # Init the neural network\n",
        "    network = FullyConnectedNet()\n",
        "\n",
        "    # Initialize optimizer\n",
        "    optimizer = optim.Adam(network.parameters(), lr=0.001)\n",
        "\n",
        "    # Run the training loop for defined number of epochs\n",
        "    for epoch in range(0, 10):\n",
        "\n",
        "        # Print epoch\n",
        "        print(f'Starting epoch {epoch+1}')\n",
        "\n",
        "        # Set current loss value\n",
        "        current_loss = 0.0\n",
        "\n",
        "        # Iterate over the DataLoader for training data\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "\n",
        "            # Get inputs\n",
        "            inputs, targets = data\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Perform forward pass\n",
        "            outputs = network(inputs)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Perform backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Perform optimization\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print statistics\n",
        "            current_loss += loss.item()\n",
        "            if i % 500 == 499:\n",
        "                print('Loss after mini-batch %5d: %.3f' %\n",
        "                      (i + 1, current_loss / 500))\n",
        "                current_loss = 0.0\n",
        "\n",
        "    # Process is complete.\n",
        "    print('Training process has finished.')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
